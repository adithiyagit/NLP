# -*- coding: utf-8 -*-
"""nlp_CADL_activities.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19N4k_v7iu2dgCjaBfov6W-p_cugTSxJm
"""

# CADL1: Preprocessing Steps
# Import libraries
import nltk
import spacy
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Download NLTK resources (run once)
nltk.download('punkt')
nltk.download("punkt_tab")  # punkt tables (new requirement in recent NLTK versions)
nltk.download('stopwords')
nltk.download('wordnet')

# Sample text corpus
corpus = [
    "Major technology companies are investing heavily in artificial intelligence.",
    "People are posting about climate change and sustainable living on social media.",
    "Morning yoga and high-protein diets are trending among fitness enthusiasts."
]

# Tokenization (NLTK)
print("ðŸ”¹ Tokenization")
for text in corpus:
    tokens = word_tokenize(text)
    print(f"Original: {text}")
    print(f"Tokens: {tokens}\n")

# Stopword removal (NLTK)
stop_words = set(stopwords.words('english'))
print("ðŸ”¹ Stopword Removal")
for text in corpus:
    tokens = word_tokenize(text)
    filtered = [w for w in tokens if w.lower() not in stop_words]
    print(f"Filtered Tokens: {filtered}\n")

# Stemming (NLTK - PorterStemmer)
ps = PorterStemmer()
print("ðŸ”¹ Stemming")
for text in corpus:
    tokens = word_tokenize(text)
    stemmed = [ps.stem(w) for w in tokens]
    print(f"Stemmed: {stemmed}\n")

# Lemmatization (NLTK - WordNetLemmatizer)
lemmatizer = WordNetLemmatizer()
print("ðŸ”¹ Lemmatization")
for text in corpus:
    tokens = word_tokenize(text)
    lemmatized = [lemmatizer.lemmatize(w) for w in tokens]
    print(f"Lemmatized: {lemmatized}\n")

# Lemmatization with spaCy
nlp = spacy.load("en_core_web_sm")
print("ðŸ”¹ Lemmatization with spaCy")
for text in corpus:
    doc = nlp(text)
    lemmas = [token.lemma_ for token in doc]
    print(f"spaCy Lemmas: {lemmas}\n")

# CADL2: Feature Extraction
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/IMDB Dataset.csv")

# Preview
print(df.head())

# Take a sample of 10 reviews for quick testing
docs = df['review'].sample(10, random_state=42).tolist()

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# Bag of Words
print("ðŸ”¹ Bag of Words Representation")
vectorizer = CountVectorizer()
X_bow = vectorizer.fit_transform(docs)
print(vectorizer.get_feature_names_out())
print(X_bow.toarray())

# TF-IDF
print("\nðŸ”¹ TF-IDF Representation")
tfidf = TfidfVectorizer()
X_tfidf = tfidf.fit_transform(docs)
print(tfidf.get_feature_names_out())
print(X_tfidf.toarray())

# CADL3: Named Entity Recognition
import spacy
import pandas as pd

nlp = spacy.load("en_core_web_sm")

# Example dataset: job postings
text = """
OpenAI is hiring Machine Learning Engineers in San Francisco.
Google announced a new AI research center in London.
Dr. Smith from MIT collaborated with Microsoft Research.
"""

doc = nlp(text)

# Extract named entities
print("ðŸ”¹ Named Entities")
for ent in doc.ents:
    print(ent.text, ent.label_)

# Structured Information (Persons and Organizations)
entities = [(ent.text, ent.label_) for ent in doc.ents]
df = pd.DataFrame(entities, columns=["Entity", "Type"])
print("\nStructured Info:\n", df)

!pip install gensim
!pip install pyLDAvis

# CADL4: Topic Modeling with LDA
import gensim
from gensim import corpora
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

# Sample text corpus
corpus = [
    "AI and machine learning are revolutionizing healthcare.",
    "Renewable energy sources are key to fighting climate change.",
    "The stock market fluctuates based on economic and political news.",
    "Yoga and meditation are becoming popular for mental health.",
    "Mars missions are advancing space exploration and technology."
]

# Preprocessing
stop_words = stopwords.words('english')
texts = [[word.lower() for word in doc.split() if word.lower() not in stop_words] for doc in corpus]

# Dictionary and Corpus
dictionary = corpora.Dictionary(texts)
doc_term_matrix = [dictionary.doc2bow(text) for text in texts]

# LDA Model
lda_model = gensim.models.LdaModel(doc_term_matrix, num_topics=3, id2word=dictionary, passes=15)

# Print topics
print("ðŸ”¹ Topics Identified")
for idx, topic in lda_model.print_topics(num_words=5):
    print(f"Topic {idx}: {topic}")

# Visualization with pyLDAvis
import pyLDAvis.gensim_models as gensimvis
import pyLDAvis

pyLDAvis.enable_notebook()
vis = gensimvis.prepare(lda_model, doc_term_matrix, dictionary)
vis